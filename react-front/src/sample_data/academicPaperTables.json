[
    {
        "paper_id": 1,
        "title": "Attention Is All You Need",
        "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
        "published_date": "2017-06-12",
        "journal": "Advances in Neural Information Processing Systems",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
        "url": "https://arxiv.org/abs/1706.03762",
        "keywords": ["Attention Mechanism", "Transformer", "Neural Machine Translation"],
        "primary_category": "cs.CL"
    },
    {
        "paper_id": 2,
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
        "published_date": "2018-10-11",
        "journal": "North American Chapter of the Association for Computational Linguistics",
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
        "url": "https://arxiv.org/abs/1810.04805",
        "keywords": ["BERT", "Language Model", "Natural Language Processing"],
        "primary_category": "cs.CL"
    },
    {
        "paper_id": 3,
        "title": "GPT-3: Language Models are Few-Shot Learners",
        "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.",
        "published_date": "2020-05-28",
        "journal": "Advances in Neural Information Processing Systems",
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples.",
        "url": "https://arxiv.org/abs/2005.14165",
        "keywords": ["GPT-3", "Language Model", "Few-Shot Learning"],
        "primary_category": "cs.CL"
    },
    {
        "paper_id": 4,
        "title": "ResNet: Deep Residual Learning for Image Recognition",
        "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
        "published_date": "2015-12-10",
        "journal": "IEEE Conference on Computer Vision and Pattern Recognition",
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
        "url": "https://arxiv.org/abs/1512.03385",
        "keywords": ["ResNet", "Deep Learning", "Computer Vision"],
        "primary_category": "cs.CV"
    },
    {
        "paper_id": 5,
        "title": "Generative Adversarial Networks",
        "authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
        "published_date": "2014-06-10",
        "journal": "Advances in Neural Information Processing Systems",
        "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.",
        "url": "https://arxiv.org/abs/1406.2661",
        "keywords": ["GAN", "Generative Model", "Deep Learning"],
        "primary_category": "cs.LG"
    },
    {
        "paper_id": 6,
        "title": "Adam: A Method for Stochastic Optimization",
        "authors": "Diederik P. Kingma, Jimmy Ba",
        "published_date": "2014-12-22",
        "journal": "International Conference on Learning Representations",
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.",
        "url": "https://arxiv.org/abs/1412.6980",
        "keywords": ["Adam", "Optimization", "Machine Learning"],
        "primary_category": "cs.LG"
    },
    {
        "paper_id": 7,
        "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
        "authors": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov",
        "published_date": "2014-06-15",
        "journal": "Journal of Machine Learning Research",
        "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time.",
        "url": "https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf",
        "keywords": ["Dropout", "Regularization", "Neural Networks"],
        "primary_category": "cs.LG"
    },
    {
        "paper_id": 8,
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "authors": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
        "published_date": "2016-05-09",
        "journal": "IEEE Conference on Computer Vision and Pattern Recognition",
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities.",
        "url": "https://arxiv.org/abs/1506.02640",
        "keywords": ["YOLO", "Object Detection", "Real-time"],
        "primary_category": "cs.CV"
    }
]